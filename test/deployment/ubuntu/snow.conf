#########################################################################################
# sNow! config file : ${SNOW_ETC}/snow.conf
# Copyright (C) 2008 Jordi Blasco
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# sNow! Cluster Suite is an opensource project developed by Jordi Blasco <jordi.blasco@hpcnow.com>
# For more information: www.hpcnow.com/snow
#
#                 [ sNow & NFS Server ]
#
# The paths where the different parts of the snow suite will be stored.
# At the time of writing this guide, some of them are hardcoded so it is
# not a good idea to change them unles you know exactly what you are
# doing
#

SNOW_PATH=/sNow
SNOW_HOME=/home
SNOW_SOFT=${SNOW_PATH}/easybuild
SNOW_CONF=${SNOW_PATH}/snow-configspace
SNOW_TOOL=${SNOW_PATH}/snow-tools

#
# Path where snow writes it's log. Rember to keep the 600 permisions
# on it as it may contain plain text passwords
#

SNOW_LOG=${SNOW_PATH}/log
LOGFILE=${SNOW_LOG}/snow.log

#
# The SNOW_PATH will be exported to the compute nodes, during deploy
# and also during the normal use of the compute cluster. Usually the
# NFS server is the first sNow server, but you can change this behaviour.
#

NFS_SERVER=beegfs01

#                 [ Virtualization Technology ]
# The current and default virtualization technology for sNow! domains is XEN.
# LXD and DOCKER support require complex manual intervention and it's still experimental
# Please, don't change this parameter unless you know what you are doing.
VIRT_TECH=XEN

#                 [ LVM disk management ]
#
# Defines where the domains OS files will be stored. This allows you 
# to use root and swap filesystems for the domains in LVM volumes or loopback 
# files. 
# The NFSROOT option requires complex manual intervention and it is still
# experimental.
#

# LVM: The domains will be stored inside a Logical Volume created in the snow_vg
# Volume Group. 
#IMG_DST='lvm=snow_vg'

# Loopback file: The domains will be stored inside a loopback file located
# inside /sNow/domains folder.
IMG_DST='dir=/sNow/domains'

# NFSROOT: The domains file system will be stored and served through a NFS
# server
#IMG_DST='nfs=$NFS_SERVER:/sNow/domains' # (experimental)


#                [ sNow! Management Servers ]
#
# Defines the list of sNow! management servers. The default value is snow01.
# If you use more than one sNow! management server then you should define here
# the list of hostnames in a blank separated list.
#
# Example of a system with two sNow! management servers:
SNOW_NODES=( snowha01 snowha02 )
#
# Example of a system with only one sNow! management server:
#SNOW_NODES=( snow01 )


#               [ Admin Users ]
#
# The list of users and groups that will have administration rights on the
# cluster. Administrators will be able to scalate privileges to root and
# perform actions like deploying/boot/shutdown domains or nodes.
#
# Note that no password is generated for those users. The only way to access to
# the system with these users is with a SSH key.
# If you preffer, you can manually setup a password with: passwd snow

ADMIN_USERS="root snow"
ADMIN_GROUPS="root snow"
MASTER_PASSWORD='HPCN0w!!'
sNow_GID=2000
sNow_GROUP=snow
sNow_UID=2000
sNow_USER=snow

# By default, there is no support from HPCNow! and the hpcnow user is not created.
# Uncomment and update the following parameters if you have already arranged
# support from sNow! developers.
#HPCNow_Support=none
#HPCNow_UID=2001
#HPCNow_GID=2000
#HPCNow_USER=hpcnow
#HPCNow_GROUP=snow

#
#         [ Source Control and Continuous Integration Support ]
# The following parameters allows to integrate the key configuration files,
# deployment scripts and other key codes located in /sNow/snow-configspace with
# your source control system. It supports GitHub and BitBucket through OAuth
# tokens. The default values are empty.
#
# This is key to enable Continuous Integration Support and test changes in a
# testing environment before to merge them into the production one.
# Since the data contained in this folder is extremely sensitive, the GIT
# repository MUST be private. Since BitBucket allows to use private
# repositories for free, we suggest to explore this option. More information
# about how to setup OAuth integrated applications is available in the GitHub
# and BitBucket websites.
#
# Example:
# PRIVATE_GIT_TOKEN=t54skl3333xxxxxxxxxxyyyy3333333srgrafsiJ
# PRIVATE_GIT_REPO=bitbucket.org/YOUR_ACCOUNT/snow-configspace.git
#

#                 [ Network definitions ]
#
# The NET_* variables define the different networks on the sNow cluster.
#
# All the networks are defined in the form:
# Network Setup [ Bridge, Gateway, First IP, Network, Netmask , Hostname extension ]
#
# Where:
#      Bridge:    The name of the network bridge associated to this network
#      Gateway:   The default GW for this network
#      First IP:  The first IP address in the rank
#      Network:   The first three octets of the network followed by a dot
#                 192.168.1. for /24 networks
#                 172.16.0.  for /16 networks
#      Netmask:   The network mask for this network
#
#      Hostname
#      extension: The extension used for name resolution, for example you
#                 may want n001 to resolve to the ETH and n001-ib to resolve
#                 to the IB network.
#

# The NET_PUB network is the external network, usually available only in
# the snow admin nodes.

NET_PUB=( 'xpub0' 'dhcp' '' '' '' '-pub' )

# The NET_SNOW network is the cluster management internal network. It is
# usually a 1 GbE network used for deploy, SSH among nodes, BATCH manager
# control of the nodes, etc.

NET_SNOW=( 'xsnow0' '10.1.0.1' '10.1.0.1' '10.1.0.' '255.255.0.0' '' )

# The NET_COMP network is a subnetwork of NET_SNOW which only contains compute nodes.
# This network is managed by the sNow CLI through the DHCP server.
#
NET_COMP=( 'xsnow0' '10.1.0.1' '10.1.1.1' '10.1.1.' '255.255.0.0' '' )

# The NET_MGMT network is the network of the baseboard management interfaces.
# Depending on which hardware you have acquired they are called IPMI (generic)
# or iLO (HP) or iDRAC (Dell) or IMM  (IBM/Lenovo).

NET_MGMT=( 'xmgmt' '10.0.0.1' '10.0.1.1' '10.0.0.' '255.255.0.0' '-mgmt' )

# The NET_LLF is the low latency fabric network. If you don't use an Infiniband,
# OPA or other high speed network, just comment the following line:

NET_LLF=( 'xllf0' '10.2.0.1' '10.2.1.1' '10.2.0.' '255.255.0.0' '-ib' )

# The NET_DMZ network is an optional network which allows to expose some
# services to the public network. 
# It may require some rules applied in your institutional firewall or to setup
# a local firewall in the sNow! management server.
# If you want to use it uncomment it and check the sNow! Administration Guide
# on how to use it.

#NET_DMZ=( 'xdmz0' '172.16.1.254' '172.16.1.1' '172.16.1.' '255.255.255.0' '-dmz' )


#                 [ IPMI Access ]
#
# Please define here the access type for the IPMI/iLO/iDRAC/IMM of your system
# and the username and password which allow interaction with it. In the present
# version of sNow! all the nodes must have the same user/pass combination.
#

IPMI_TYPE=lanplus
IPMI_USER=USERID
IPMI_PASSWORD=PASSW0RD

#                 [ Boot delay ]
#
# When booting a big cluster it is usually not a good idea to power on tens or
# hundreds of nodes at the same time. The sNow! approach is to boot BLOCKN nodes
# every BLOCKD seconds. Please define your preference here or leave untouched.
# The default setup allows to boot 12 nodes every 5 seconds.
#

BLOCKN=12
BLOCKD=5

# When you deploy a node sNow! prepares it to boot via PXE. After BOOT_DELAY
# seconds the node will be configured back to boot from it's default boot mechanism.
# The time in this variable should be enough for the node to PXE boot and start
# the OS installation but short enough to finish during installation.
# The default value is 300 seconds.

BOOT_DELAY=300

#                [ Configuration Manager ]
#
# sNow! allows to integrate your preferred configuration manager. In the case
# you are using CFEngine, there is a role which partially integrates it. If you
# are using another one, maybe you want to create a new role. Since the
# configuration managers are quite complex to setup and it also strongly
# depends on the site implementation, this component is outside the sNow! scope
# and support. sNow! only provides integration but not deployment of this
# service. The default values are unset.
#
# CM_SOFTWARE: Defines the name of the configuration manager to be used.
# Default value is empty.
#CM_SOFTWARE=
#
# CM_SERVER: Defines the configuration manager server. Default value is empty.
#CM_SERVER=
#
# CM_VERSION: Defines the version of the configuration manager to be used.
# Default value is empty.
#CM_VERSION=

#                 [ Template management ]
#
# sNow! template based deployment supports having different templates for
# different purposes. This is the default template that will be used if you don't
# specify any, for example, on a node deploy. You can change this value every
# time you want during the cluster lifetime.
# You can also setup different template for a specific node through snow CLI.

DEFAULT_BOOT=localboot
DEFAULT_TEMPLATE=centos-7.4-default

# Some compute nodes may require different options in order to interact with
# the remote console during the PXE boot. This option is used as default.
# You can also setup different console options for a specific node through snow CLI.
DEFAULT_CONSOLE_OPTIONS="console=tty0 console=ttyS0,115200n8"

#                 [ Cluster definition ]
#
# sNow! has multi-clustering support. You can define your clusters as a bash
# array format.
#
# The following example defines three clusters and their associated compute nodes.
#CLUSTERS=([bdw]="bdw[01-99]" [mycluster02]="hsw[01-99]" [mycluster03]="skl[01-99]")
#
# The following example defines a single cluster and its associated compute nodes.
CLUSTERS=([bdw]="bdw[01-04]")

#
# When setting up a cluster there are actions which only need to be performed
# once per cluster. The "golden nodes" define from which nodes these actions
# are performed.
# In the case of very heterogeneous architectures allocated in the same cluster,
# these actions may required to be performed once per architecture.
# This is for example the case of EasyBuild installation which will require to
# be run only once per each architecture.
#
# If you have a standard cluster just write down the hostname of the first node
# here. If you have multiple clusters or a mix of architectures please write down
# the hostname of the first node of each cluster/architecture.
#

GOLDEN_NODES=( bdw-01 )

#
#                [ Downloader ]
# The main downloader tool in sNow! is wget, but we recommend to try axel,
# which allows to accelerate the process quite significantly compared to
# standard wget. There are three options available axel, curl and wget.
# This software is used to download large files like the domain image template,
# OS images or PXE kernels.
#

DOWNLD=wget


#                [ Locales ]
# The nodes and domains managed by sNow! requires consistent locales
# configuration.
# The following options ensure the consistency across the services allocated in
# the domains and the nodes deployed with sNow!
# 

LANG=en_US
KEYMAP=us
TIMEZONE=Europe/Madrid

#             [ DNS options ]
#
# Network parameters and services required by sNow! (mandatory)
# The DNS servers are used by the role deploy to setup the internal DNS.
# The DNS search list (DNS_SEARCH_LIST) allows to search across multiple domains.
# This option is currently limited to six domains with a total of 256 characters.
#

DNS_SERVERS=8.8.8.8,8.8.4.4
DOMAIN=in.hpcnow.com
#DNS_SEARCH_LIST="mycluster.mydomain.org mydomain.net someotherdomain.com"

#             [ DHCP options ]
#
# The sNow! domain NIC that will be used for serving DHCP and PXE to the
# compute nodes. If your sNow! implementation supports eIPoIB and your compute
# nodes allows to boot from PXE over Infiniband, you may want to update this
# value in order to accelerate the booting time.
#
DHCP_NIC=eth0

#             [ (optional) Site Network Services ]
#
# sNow! provides easy integration with your existing environment through the
# SITE_SERVICES variables. The following parametres define integration with
# standard services usually required by an HPC cluster. Other services can be
# integrated via hooks (see section 12 of sNow! administrator documentation). 
#

#SITE_PROXY_SERVER=192.168.7.1
#SITE_PROXY_PORT=8080
#SITE_NTP_SERVER=192.168.7.1
#SITE_LDAP_SERVER=192.168.7.1
#SITE_LDAP_URI="ldap://ldap01.hpcnow.com, ldap://ldap02.hpcnow.com"
#SITE_LDAP_TLS=FALSE
#SITE_LDAP_PROTO=ldap
#SITE_LDAP_BASE="dc=in,dc=hpcnow,dc=com"
#SITE_MAIL_SERVER=smtp.gmail.com::587
#SITE_MAIL_USER=
#SITE_MAIL_PASSWORD=
#SITE_SYSLOG_SERVER=192.168.7.1

#             [ Shared File System ]
#
# sNow! supports integration with the most popular shared filesystems in HPC
# environments via hooks.
# sNow! has native support for NFS clients and the NFS mounts can be defined like this:
# MOUNT_NFS[3]="my-nfs-server:/projects         /projects      nfs    bg,tcp,defaults 0 0"

# If NFS is your main shared file system, then the following two lines become
# mandatory, as they define the mount points for $SNOW_PATH and $HOME
MOUNT_NFS[1]="${NFS_SERVER}:${SNOW_PATH}        ${SNOW_PATH}   nfs    bg,tcp,noatime,nodiratime,defaults 0 0"
MOUNT_NFS[2]="${NFS_SERVER}:${SNOW_HOME}        ${SNOW_HOME}   nfs    bg,tcp,noatime,nodiratime,defaults 0 0"

# If you are using a different shared file system, then the $SNOW_PATH and
# $HOME must be available in the sNow! management nodes and some node(s) must
# re-export those file systems through NFS. 
#
# NFS Performance considerations:
# * Consider to increase the number of NFS daemons (RPCNFSDCOUNT) in the server
# * Consider to include FS-Cache option in the clients (fsc)
# * Consider to disable async in the ${SNOW_PATH} exports (highly recommended if you don't trust the hardware)

#             [ Slurm Configuration ]
#
# The following parameters will help you to setup and configure a ready for
# production Slurm Workload Manager. For more information regarding user point
# of view, please visit the Slurm website.
#

# The following parameters define the Slurm Database
SLURMDBD_USER=slurm
SLURMDBD_PASSWORD=$MASTER_PASSWORD
#SLURMDBD_NAME=slurm_acct_db

#             [ Slurm Accounting and Fair Share ]
#               
# The following option forces to use associations and QoS for accounting.
# Fairsharing is the most common approach to share resources. Consider to use
# the helper script "$SNOW_TOOLS/contrib/slurm_fairshare/slurm_share_tree.sh"
# to define how to share the computational resources within your groups and users 
#ACCOUNTING_STORAGE_ENFORCE=associations,qos
#
# nojobs - This will make it so no job information is stored in accounting.
# nosteps - This will make it so no step information is stored in accounting.
# Both nojobs and nosteps could be helpful in an environment where you want to
# use limits but don't really care about utilization.
#
# More information here: https://slurm.schedmd.com/accounting.html
ACCOUNTING_STORAGE_ENFORCE=nojobs
SLURM_CLUSTER_NAME=bdw

# MUNGE User definition
MUNGE_UID=20994
MUNGE_GID=20994

# The following parameters define the Slurmctld
#SLURM_VERSION=15.8.2
SLURM_CONF=/etc/slurm/slurm.conf
# Slurm User definition
SLURM_GID=20995
SLURM_UID=20995
# The following parameter defines the number of licenses available for each
# software. 
# Syntax: LICENSES=licenseA*number_of_tockens,licenseB*number_of_tockens
# Example: LICENSES=intel*2,fluent*5000
LICENSES=intel*2
# The following lines define three different types of Slurm Compute Nodes 
SLURM_NODES[1]="NodeName=bdw[01-04] RealMemory=64000  Sockets=2  CoresPerSocket=12 ThreadsPerCore=1 State=UNKNOWN" 
# The following lines define four simple partitions which meet most of the
# requirements in small HPC centers
SLURM_PARTITION[1]="PartitionName=high    Nodes=bdw[01-04]  Default=NO  OverSubscribe=FORCE:1 Priority=100 PriorityTier=5 MaxTime=6:00:00   PreemptMode=off"
SLURM_PARTITION[2]="PartitionName=medium  Nodes=bdw[01-04]  Default=NO  OverSubscribe=FORCE:1 Priority=75  PriorityTier=1 MaxTime=72:00:00  PreemptMode=off"
SLURM_PARTITION[3]="PartitionName=requeue Nodes=bdw[01-04]  Default=NO  OverSubscribe=NO      Priority=50  PriorityTier=3 MaxTime=24:00:00  PreemptMode=requeue    GraceTime=120"
SLURM_PARTITION[4]="PartitionName=low     Nodes=bdw[01-04]  Default=YES OverSubscribe=FORCE:1 Priority=25  PriorityTier=2 MaxTime=168:00:00 PreemptMode=suspend"

#             [ Torque Configuration ]
# sNow! allows to install a torque master domain and also build the required packages to 
# be deployed in the compute nodes. The following two parameters will help you to define the very basic
# configuration of torque. Maui is not supported

#TORQUE_VERSION=6.1.1.1

#             [ LDAP Configuration ]
# sNow! can deploy and integrate LDAP server(s). The following parameters allow
# you to setup the service with a minor overhead.

# LDAP Passwords
# LDAP_ADMIN_PASSWORD: Ldap Admin password. Defaults to admin
# LDAP_CONFIG_PASSWORD: Ldap Config password. Defaults to config
LDAP_ADMIN_PASSWORD=$MASTER_PASSWORD
LDAP_CONFIG_PASSWORD=$MASTER_PASSWORD

# LDAP organization name. By default is the domain name defined with $DOMAIN
#LDAP_ORGANIZATION="name of your site"

# Additional policies and schemas
#LDAP_PPOLICY_DN_PREFIX=cn=default,ou=policies
#LDAP_ADDITIONAL_SCHEMAS=collective,corba,duaconf,dyngroup,java,misc,openldap,pmi,ppolicy
#LDAP_ADDITIONAL_MODULES=memberof,ppolicy
